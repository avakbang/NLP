# -*- coding: utf-8 -*-
"""everything,_everywhere,_all_at_once.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1otk0TOCw3dxEQi2HBo6e-PpQQw4NgzIz

# Loading Libraries and Data
"""

! pip install contractions
! pip install pyspellchecker
! pip install spacy
! python -m spacy download en_core_web_sm
! pip install tensorflow
! pip install keras
! pip install -q tensorflow seaborn spacy
! pip install imblearn
! pip install tf_keras
! pip install wordcloud

# Other
import os
import pathlib
import kagglehub
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# Preprocessing
import re
import contractions
from collections import Counter
from spacy.lang.en.stop_words import STOP_WORDS
import spacy
nlp = spacy.load("en_core_web_sm")
from spellchecker import SpellChecker

# Classical Models
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    precision_recall_fscore_support
)
from imblearn.over_sampling import ADASYN, SMOTE

# Neural Networks
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import pad_sequences
from tensorflow.keras.layers import (
    Input, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense,
    Bidirectional, GRU, concatenate
)
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
#! wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip
#! unzip glove.6B.zip -d glove6B

import requests, zipfile, io
url = "http://nlp.stanford.edu/data/glove.6B.zip"
r = requests.get(url)
with zipfile.ZipFile(io.BytesIO(r.content)) as zip_ref:
    zip_ref.extractall("glove6B")

# BERT
import torch
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    Trainer, TrainingArguments
)
from datasets import Dataset

import kagglehub

# Download latest version
path = kagglehub.dataset_download("meetnagadia/amazon-kindle-book-review-for-sentiment-analysis")

print("Path to dataset files:", path)

df = pd.read_csv(f"{path}/all_kindle_review .csv")

"""# Preprocessing

## Sentiment Mapping

Map sentinment scores into labels. As seen in the histogram beneath there are mostly positive reviews, followed by negative reviews, and neutral reviews being the least common.
"""

def map_sentiment(rating):
  # Ratings with 1 or 2 will be labeled as negative
  if rating in [1, 2]:
    return 'negative'
  # Ratings with 3 will be labeled as neutral
  elif rating == 3:
    return 'neutral'
  # Ratings with 4 or 5 will be labeles as positive
  elif rating in [4, 5]:
    return 'positive'
  else:
    return None

# Use sentiment instead of rating
df['sentiment'] = df['rating'].apply(map_sentiment)

# Find sentiment distribution
sentiment_counts = df['sentiment'].value_counts().reset_index()
sentiment_counts.columns = ['sentiment', 'count']

# Plot distribution in histogram
plt.figure(figsize=(8, 6))
sns.barplot(x='sentiment', y='count', data=sentiment_counts)
plt.title('Distribution of Sentiment in Reviews')
plt.xlabel('Sentiment')
plt.ylabel('Number of Reviews')
plt.show()

"""## Cleaning for Classical Models
Generally classical models perform better with extensive preprocessing. We have therefore decided to lowercase, remove contraction and expansion, correcting spellning errors, apply stopword removal, tokenize, POS tag, lemmatize and handle rare words.
"""

def classical_clean_basic(text):
  # Lowercasing text
  text = str(text).lower()
  # Handle contractions (don´t --> do not)
  text = contractions.fix(text)
  # Handle expansions (amaaaaaziiing --> amazing)
  text = re.sub(r'(.)\1{2,}', r'\1', text)
  return text

# Apply basic cleaning to reviewText
df['classical'] = df['reviewText'].apply(classical_clean_basic)

"""Originally, 11% of all words are misspelled. This is bordeline high. Classical models such as Naive Bayes and Logistic Regression handles them poorly. Correcting all spelling errors proved too time-consuming, so we chose to correct only the top 1000 most frequent misspellings instead."""

# Set up spell checker object
spell = SpellChecker()
# Empty list for misspelled words
misspelled_words = []

# Function to count misspellings in a string
def count_misspellings(text):
    words = str(text).split()
    misspelled = spell.unknown(words)
    misspelled_words.extend(misspelled)
    return len(misspelled)

# Adds a new column with count of misspellings in each row
df['misspelled_count'] = df['classical'].apply(count_misspellings)

# Compute overall statistics
total_misspellings = df['misspelled_count'].sum()
total_words = df['classical'].apply(lambda x: len(str(x).split())).sum()
misspelled_percentage = (total_misspellings / total_words) * 100

# Print results
print(f"Total misspelled words: {total_misspellings:,}")
print(f"Misspelled word rate: {misspelled_percentage:.2f}%")

# Build correct dictionary
top_misspellings = dict(Counter(misspelled_words).most_common(1000))
correction_dict = {word: spell.correction(word) for word in top_misspellings if spell.correction(word)}

# Correction function
def correct_text(text):
    return " ".join(correction_dict.get(word, word) for word in str(text).split())

# Apply correction
df['classical'] = df['classical'].apply(correct_text)

"""After correcting the top 1000 most frequent spelling errors, the misspelled word rate dropped to just under 7%.

We decided to use **lemmatization over stemming** because:
- lemmatization is more accurate and preserves true word meaning
- handles complex forms accurately
- avoids inccorect cuts
</b>

We decided to include **Part-Of-Speech (POS) tagging**, which assigns grammatical labels to each word. Especially helpfull to improve lemmatization quality and focuses on high-sentiment part os speech.
</b>
"""

# Loads english language model
nlp = spacy.load("en_core_web_sm")
# Assigns default english stop words from spacy to variable
stop_words = STOP_WORDS

def classical_clean_advanced(text):
  # Parse text into tokens
  doc = nlp(str(text))

  # Filter out punctuation, stopwords, and short tokens
  tokens = [token for token in doc if token.is_alpha and (len(token.text) > 1)]

  # Remove stopwords
  tokens_nostop = [token for token in tokens if token.text not in stop_words]

  # Select meaningful POS - keeps only NOUNs, VERBs, ADJs and ADV
  tokens_nv = [token.text for token in tokens_nostop if token.pos_ in {"NOUN", "VERB", "ADJ", "ADV"}]

  # Lemmatize words to base form
  lemmatized = [token.lemma_ for token in tokens_nostop if token.pos_ in {"NOUN", "VERB", "ADJ", "ADV"}]

  # Save POS tagging
  pos_tags = [token.pos_ for token in tokens]

  # Adds lemmatized and pos
  return pd.Series([lemmatized, pos_tags], index=["lemmatized", "pos_tags"])

df_cleaned = df.join(df['classical'].apply(classical_clean_advanced))

# Count lemmatized words
all_words = [word for row in df_cleaned['lemmatized'] for word in row]
word_freq = Counter(all_words)

# Define rare words
rare_words = {word for word, count in word_freq.items() if count <= 3}

# Compute original length
df_cleaned['original_len'] = df_cleaned['lemmatized'].apply(len)

# Removes rare words and updating the 'classical' column
df_cleaned['classical'] = df_cleaned['lemmatized'].apply(
    lambda row: [w for w in row if w not in rare_words])

# Track how many words were removed
df_cleaned['filtered_len'] = df_cleaned['classical'].apply(len)
df_cleaned['count'] = df_cleaned['original_len'] - df_cleaned['filtered_len']

# Print results
print(df_cleaned['count'].describe())

"""## Cleaning for Neural Networks
As Neural Networks need minimal cleaning. We have tokenized, lowercased and added padding.
"""

# Tokenize, normalize and lowercase using spaCy pipeline batching
def basic_clean_pipe(docs):
    for doc in docs:
        yield " ".join(tok.lemma_ for tok in doc if tok.is_alpha and not tok.is_stop)

# Fill NAs and convert to lowercase
texts = df['reviewText'].fillna("").str.lower().tolist()

# Apply nlp.pipe with batching
cleaned_texts = list(basic_clean_pipe(nlp.pipe(texts, batch_size=1000)))

# Assign cleaned text back to the DataFrame
df['clean_nn'] = cleaned_texts

# Fit vocabulary
tok = Tokenizer(oov_token="<UNK>")
tok.fit_on_texts(df['clean_nn'])

# Turn text into integer IDs
seqs = tok.texts_to_sequences(df['clean_nn'])
lengths = np.fromiter((len(s) for s in seqs), dtype=np.int32)
MAX_LEN = int(np.percentile(lengths, 95))

X = pad_sequences(seqs, maxlen=MAX_LEN,
                  padding="post", truncating="post")

"""## Cleaning for Bert
As BERT performs best without any cleaning. There is not performed any.
"""

df = df.dropna(subset=['sentiment', 'reviewText'])

"""## Visualizations
As seen in the visualizations there are some words which are frequently used in book reviews that does not give any valuable insight. </b>

For more advanced NLP/ML algorithms, instead of removing these words, wordembedding is used to capture meaning and relationships. Word embedding methods include GloVe and Word2Vec.

**POS Distribution**
</b>

- Noun-heavy dataset - reviews mention object, topics people and things a lot
- verbs are frequent - reviews express actions, opinions and experiences
- Pronouns and puncutation are common - shows that reviews are written informal and personal
- Adjectives and adverbs are present but smaller - usually carry strong emotional weight and are very valuable for sentiment models
"""

# Flattens lists to into one big list of tags
all_pos = [pos for tags in df_cleaned['pos_tags'] for pos in tags]
# Counts each POS tag
pos_counts = Counter(all_pos).most_common()

# Visualize in histogram
labels, counts = zip(*pos_counts)
plt.figure(figsize=(8,5))
plt.bar(labels, counts)
plt.title("POS Tag Distribution")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.show()

"""**Most Common Lemmas**
</b>

This shows us that the preprocessing pipeline works. We see words such as "read" instead of "reads" or "reading". Also common stopwords like "the", "was" and "is" are absent. However the dataset has a high frequency of domain-specific words such as "book", "story" and "character". This does not carry sentimen on their own. These should be downeight or excluded when building features.
"""

# Flatten list of all lemmas
all_lemmas = [lemma for lemmas in df_cleaned['lemmatized'] for lemma in lemmas]

# Count top 20
lemma_freq = Counter(all_lemmas).most_common(20)

# Visualize in histogram
words, freqs = zip(*lemma_freq)
plt.figure(figsize=(10,5))
plt.bar(words, freqs)
plt.xticks(rotation=45)
plt.title("Top 20 Most Common Lemmas")
plt.ylabel("Frequency")
plt.show()

"""**Word Clouds Per Sentiment**
</b>

As notet in Most Common Lemmas there are words which are too general for modeling which reduces the value of the illustrations. This is noted for feature selection.
"""

# Filters rows by sentiment and joins all words into a single string
def get_text_by_sentiment(df, sentiment):
  return " ".join(
      word for word in df[df['sentiment'] == sentiment]['lemmatized'].explode())

# Visualize in word cloud
for sentiment in ['positive', 'neutral', 'negative']:
  text = get_text_by_sentiment(df_cleaned, sentiment)
  wc = WordCloud(width=800, height=400).generate(text)
  plt.figure(figsize=(10, 5))
  plt.imshow(wc, interpolation='bilinear')
  plt.axis('off')
  plt.title(f"Word Cloud for {sentiment.capitalize()} Reviews")
  plt.show()

"""# Train-Test Split"""

# Encode sentiment labels to integers
label_map = {'negative': 0, 'neutral': 1, 'positive': 2}
df['labels'] = df['sentiment'].map(label_map)

# Train test split for NN
X_train, X_val, y_train, y_val = train_test_split(
    X,
    df['labels'].values,
    test_size=0.2,
    stratify=df['labels'].values,
    random_state=42
)

# Train-val split for BERT
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['reviewText'].tolist(),
    df['labels'].tolist(),
    test_size=0.2,
    stratify=df['labels'],
    random_state=42
)

"""# Classical Models"""

# Create dictionary for models
models = {
  "Multinomial Naive Bayes": MultinomialNB(),
  "Logistic Regression": LogisticRegression(max_iter=1000)
}

# Vectorization Function to turn text data to numerical feature vectors
def vectorize_data(X_train, X_test, method='bow'):
  vectorizer = CountVectorizer() if method == 'bow' else TfidfVectorizer()
  # Ensure X_train and X_test are lists of strings before vectorizing
  X_train_list = [str(text) for text in X_train]
  X_test_list = [str(text) for text in X_test]
  return vectorizer.fit_transform(X_train_list), vectorizer.transform(X_test_list)

# Model Training and Evaluation Function
def train_and_evaluate(name, model, X_train_vec, X_test_vec, y_train, y_test):
  print(f"\n{name}")
  model.fit(X_train_vec, y_train)
  y_pred = model.predict(X_test_vec)

  # Print results
  print(classification_report(y_test, y_pred, target_names=label_map.keys()))
  cm = confusion_matrix(y_test, y_pred)
  plt.figure(figsize=(8, 6))
  sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=list(label_map.keys()), yticklabels=list(label_map.keys()), cbar=True)
  plt.title(f"{name} Confusion Matrix")
  plt.xlabel("Predicted Label")
  plt.ylabel("True Label")
  plt.tight_layout()
  plt.show()

# Function to run experiments with different preprocessing and vectorization methods
def run_experiment(data_df, text_column, preproc_label):
  # Ensure the text column contains string data for vectorization
  X_train_text, X_test_text, y_train, y_test = train_test_split(
      data_df[text_column].astype(str), data_df['labels'],
      test_size=0.2, stratify=data_df['labels'], random_state=42
  )
  for method in ['bow', 'tfidf']:
      X_train_vec, X_test_vec = vectorize_data(X_train_text, X_test_text, method)
      for name, clf in models.items():
          train_and_evaluate(f"{preproc_label} [{method.upper()}] {name}", clf, X_train_vec, X_test_vec, y_train, y_test)

# Function to run experiments with oversampling
def run_oversampling_experiment(data_df, text_column, method='adasyn'):
  # Ensure the text column contains string data for vectorization
  X_train_text, X_test_text, y_train, y_test = train_test_split(
      data_df[text_column].astype(str), data_df['labels'],
      test_size=0.2, stratify=data_df['labels'], random_state=42
  )
  X_train_vect, X_test_vect = vectorize_data(X_train_text, X_test_text)

  sampler = ADASYN(random_state=42) if method == 'adasyn' else SMOTE(random_state=42)
  # Resample the training data only
  X_train_balanced, y_train_balanced = sampler.fit_resample(X_train_vect, y_train)

  for name, clf in models.items():
      train_and_evaluate(f"[BoW + {method.upper()}] {name}", clf, X_train_balanced, X_test_vect, y_train_balanced, y_test)


# Minimal preprocessing (which is effectively the 'classical' column after basic cleaning)
run_experiment(df, "clean_nn", "Minimal Preprocessing")

# Extensive preprocessing (which is the 'classical' column after advanced cleaning and rare word removal)
run_experiment(df, "classical", "Extensive Preprocessing")

# Oversampling: ADASYN
run_oversampling_experiment(df, "classical", method='adasyn')

# Oversampling: SMOTE
run_oversampling_experiment(df, "classical", method='smote')

"""# Neural Network Models

## GloVe
"""

# Each word will be represented as a 100-dimensional vector
EMBED_DIM = 100
# File path to GloVe
glove_file = pathlib.Path("glove6B") / f"glove.6B.{EMBED_DIM}d.txt"

# Reads GloVe file and creates dictionary and maps each word to it´s corresponding embedding vector
emb_index = {}
with glove_file.open(encoding="utf-8") as f:
  for line in f:
    word, *vec = line.strip().split()
    emb_index[word] = np.asarray(vec, dtype="float32")

# Map words to their integer indices
vocab_size = len(tok.word_index) + 1
# Initialize embedding with random values
emb_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, EMBED_DIM))

# For each word in the vocabulary it tries to find its pre-trained vector
for word, idx in tok.word_index.items():
  vec = emb_index.get(word)
  if vec is not None:
    emb_matrix[idx] = vec

"""## CNN"""

# Takes input sequens of integers with fixed length
inp_cnn = Input(shape=(MAX_LEN,))

# Converts each word index into its vector representation
emb_cnn = Embedding(vocab_size, EMBED_DIM,
                weights=[emb_matrix],
                input_length=MAX_LEN,
                # Model can fine-tune embeddings during training
                trainable=True)(inp_cnn)

# Apply 1D convolution filters of size 2-6
conv2 = Conv1D(256, 2, activation="relu", padding="same")(emb_cnn)
conv3 = Conv1D(256, 3, activation="relu", padding="same")(emb_cnn)
conv4 = Conv1D(256, 4, activation="relu", padding="same")(emb_cnn)
conv5 = Conv1D(256, 5, activation="relu", padding="same")(emb_cnn)
conv6 = Conv1D(256, 6, activation="relu", padding="same")(emb_cnn)

# Randomly drops 16% of neurons during training to prevent overfitting
conv_stack = concatenate([conv2, conv3, conv4, conv5, conv6])
conv_stack = Dropout(0.16824117756047322)(conv_stack)

# Each convolution´s output is compressed to its most important feature
feat_cnn = concatenate([GlobalMaxPooling1D()(c) for c in (conv2, conv3, conv4, conv5, conv6)])
# Drops 50% of neurons to reduce overfitting before final output layer
feat_cnn = Dropout(0.5)(feat_cnn)
# Output layer with 3 unigs
# Softmax turns a list of numbers into probabilities
out_cnn  = Dense(3, activation="softmax")(feat_cnn)

# Use Adam optimizer with learning rate 0.0016
model_cnn = Model(inp_cnn, out_cnn)
model_cnn.compile(optimizer=Adam(0.0016108321063574083),
  # Used becayse labels are integers
  loss="sparse_categorical_crossentropy",
  metrics=["accuracy"])

# Print results
model_cnn.summary()

"""## CRNN"""

inp_crnn = Input(shape=(MAX_LEN,))
emb_crnn = Embedding(vocab_size, EMBED_DIM,
                     weights=[emb_matrix],
                     input_length=MAX_LEN,
                     trainable=True)(inp_crnn)

conv2 = Conv1D(160, 2, activation="relu", padding="same")(emb_crnn)
conv3 = Conv1D(160, 3, activation="relu", padding="same")(emb_crnn)
conv4 = Conv1D(160, 4, activation="relu", padding="same")(emb_crnn)
conv5 = Conv1D(160, 5, activation="relu", padding="same")(emb_crnn)
conv6 = Conv1D(160, 6, activation="relu", padding="same")(emb_crnn)

conv_stack = concatenate([conv2, conv3, conv4, conv5, conv6])
conv_stack = Dropout(0.3823929130402751)(conv_stack)

rnn_out = Bidirectional(
    GRU(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)
)(conv_stack)

feat_crnn = Dropout(0.5)(rnn_out)
out_crnn = Dense(3, activation="softmax")(feat_crnn)

model_crnn = Model(inp_crnn, out_crnn)
model_crnn.compile(optimizer=Adam(0.0006816949460296288),
                   loss="sparse_categorical_crossentropy",
                   metrics=["accuracy"])

"""## Train

As the dataset is imbalanced class weights is applied to make sure that the model pay attention to underrepresented classes. Neural samples will have the most influence on the luss function, helping the model not to ignore them.
"""

# Use class weights to handle class imbalance
class_weight = {0: 3.0,   # upping negative
                1: 6.0,   # upping neutral
                2: 2.0}   # positive stays baseline

# Early stopping callback
early_stop = tf.keras.callbacks.EarlyStopping(
    patience=2,
    restore_best_weights=True
)

# Train CNN model
model_cnn.fit(
    # Training data
    X_train, y_train,
    # Validation data
    validation_data=(X_val, y_val),
    # Train up to 10 rounds
    epochs=10,
    # 128 samples a a time
    batch_size=128,
    # Apply class weights
    class_weight=class_weight,
    # Apply early stop
    callbacks=[early_stop]
)

# Train CRNN model, same as CNN
model_crnn.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=10,
    batch_size=128,
    class_weight=class_weight,
    callbacks=[early_stop]
)

"""## Evaluation"""

# Evaluation function
def evaluate_model(model, model_name="Model"):
  # Predict
  y_prob = model.predict(X_val, batch_size=128, verbose=0)
  y_pred = y_prob.argmax(axis=1)

  # Scalar metrics
  acc = accuracy_score(y_val, y_pred)
  prec, rec, f1, _ = precision_recall_fscore_support(
      y_val, y_pred, average="macro", zero_division=0)

  # Print results
  print(f"\nResults for {model_name}")
  print(f"Accuracy : {acc:.4f}")
  print(f"Precision: {prec:.4f}  |  Recall: {rec:.4f}  |  F1: {f1:.4f}\n")
  print(classification_report(y_val, y_pred,
                                target_names=['negative', 'neutral', 'positive']))

  # Confusion matrix
  cm = confusion_matrix(y_val, y_pred)
  plt.figure(figsize=(8, 6))
  sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
              xticklabels=['neg', 'neu', 'pos'],
              yticklabels=['neg', 'neu', 'pos'])
  plt.title(f"{model_name} Confusion Matrix")
  plt.xlabel("Predicted"); plt.ylabel("Actual")
  plt.tight_layout(); plt.show()

# Evaluate both models
evaluate_model(model_cnn, model_name="CNN")
evaluate_model(model_crnn, model_name="CRNN")

"""# BERT

## Build
"""

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Tokenize the data
def tokenize_function(texts):
    return tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors="pt")

"""## Train"""

# Create datasets
train_encodings = tokenize_function(train_texts)
val_encodings = tokenize_function(val_texts)

# Convert to Dataset format
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels': train_labels
})

val_dataset = Dataset.from_dict({
    'input_ids': val_encodings['input_ids'],
    'attention_mask': val_encodings['attention_mask'],
    'labels': val_labels
})

# Training arguments
training_args = TrainingArguments(
    output_dir='./bert_results',
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    per_device_eval_batch_size=2,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    report_to='none'
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Train the model
trainer.train()

"""## Evaluation"""

# Evaluate
predictions = trainer.predict(val_dataset)
y_pred = np.argmax(predictions.predictions, axis=1)

# Results
print("Accuracy:", accuracy_score(val_labels, y_pred))
print("\nClassification Report:")
print(classification_report(val_labels, y_pred, target_names=['negative', 'neutral', 'positive']))

# Confusion matrix
cm = confusion_matrix(val_labels, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['negative', 'neutral', 'positive'],
            yticklabels=['negative', 'neutral', 'positive'])
plt.title('BERT Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()